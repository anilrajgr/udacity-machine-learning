{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('max_colwidth',1000)\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the loan data\n",
    "df = pd.read_pickle('data_cleaned.pkl')\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features from data-cleanup.ipynb\n",
    "cat_features = ['grade', 'sub_grade', 'emp_length', 'home_ownership', 'verification_status', \n",
    "                'purpose', 'addr_state', 'initial_list_status', 'application_type', 'disbursement_method']\n",
    "\n",
    "for y in cat_features:\n",
    "    # print(y + \" has \" + str(len(df[y].unique())) + \" unique values\")\n",
    "    df = df.join(pd.get_dummies(df[y], prefix=y))\n",
    "    df.drop(y, axis=1, inplace=True)\n",
    "\n",
    "# Remove int_rate also\n",
    "df.drop('int_rate', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the loan_status to numerical values\n",
    "# Fully Paid = 1\n",
    "# Charged Off = 0\n",
    "df['loan_status'] = df['loan_status'].apply(lambda x: int(x == 'Fully Paid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>...</th>\n",
       "      <th>addr_state_WA</th>\n",
       "      <th>addr_state_WI</th>\n",
       "      <th>addr_state_WV</th>\n",
       "      <th>addr_state_WY</th>\n",
       "      <th>initial_list_status_f</th>\n",
       "      <th>initial_list_status_w</th>\n",
       "      <th>application_type_Individual</th>\n",
       "      <th>application_type_Joint App</th>\n",
       "      <th>disbursement_method_Cash</th>\n",
       "      <th>disbursement_method_DirectPay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>8.306850e+05</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "      <td>830685.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14325.615727</td>\n",
       "      <td>14312.102722</td>\n",
       "      <td>14279.615080</td>\n",
       "      <td>41.796970</td>\n",
       "      <td>436.406142</td>\n",
       "      <td>7.529807e+04</td>\n",
       "      <td>0.793832</td>\n",
       "      <td>518.830457</td>\n",
       "      <td>17.395740</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022308</td>\n",
       "      <td>0.012747</td>\n",
       "      <td>0.004003</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.504204</td>\n",
       "      <td>0.495796</td>\n",
       "      <td>0.994908</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>0.998617</td>\n",
       "      <td>0.001383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8509.996557</td>\n",
       "      <td>8503.337725</td>\n",
       "      <td>8508.362095</td>\n",
       "      <td>10.272417</td>\n",
       "      <td>255.953656</td>\n",
       "      <td>6.557081e+04</td>\n",
       "      <td>0.404553</td>\n",
       "      <td>314.240924</td>\n",
       "      <td>8.984933</td>\n",
       "      <td>0.857069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147684</td>\n",
       "      <td>0.112182</td>\n",
       "      <td>0.063140</td>\n",
       "      <td>0.047317</td>\n",
       "      <td>0.499983</td>\n",
       "      <td>0.499983</td>\n",
       "      <td>0.071178</td>\n",
       "      <td>0.071178</td>\n",
       "      <td>0.037166</td>\n",
       "      <td>0.037166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8000.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>7975.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>4.520000e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12000.000000</td>\n",
       "      <td>12000.000000</td>\n",
       "      <td>12000.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>6.500000e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>481.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>9.000000e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>823.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40000.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1714.000000</td>\n",
       "      <td>9.550000e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           loan_amnt    funded_amnt  funded_amnt_inv           term  \\\n",
       "count  830685.000000  830685.000000    830685.000000  830685.000000   \n",
       "mean    14325.615727   14312.102722     14279.615080      41.796970   \n",
       "std      8509.996557    8503.337725      8508.362095      10.272417   \n",
       "min       500.000000     500.000000         0.000000      36.000000   \n",
       "25%      8000.000000    8000.000000      7975.000000      36.000000   \n",
       "50%     12000.000000   12000.000000     12000.000000      36.000000   \n",
       "75%     20000.000000   20000.000000     20000.000000      36.000000   \n",
       "max     40000.000000   40000.000000     40000.000000      60.000000   \n",
       "\n",
       "         installment    annual_inc    loan_status       zip_code  \\\n",
       "count  830685.000000  8.306850e+05  830685.000000  830685.000000   \n",
       "mean      436.406142  7.529807e+04       0.793832     518.830457   \n",
       "std       255.953656  6.557081e+04       0.404553     314.240924   \n",
       "min         4.000000  0.000000e+00       0.000000       7.000000   \n",
       "25%       251.000000  4.520000e+04       1.000000     234.000000   \n",
       "50%       377.000000  6.500000e+04       1.000000     481.000000   \n",
       "75%       576.000000  9.000000e+04       1.000000     823.000000   \n",
       "max      1714.000000  9.550000e+06       1.000000     999.000000   \n",
       "\n",
       "                 dti    delinq_2yrs              ...                \\\n",
       "count  830685.000000  830685.000000              ...                 \n",
       "mean       17.395740       0.307000              ...                 \n",
       "std         8.984933       0.857069              ...                 \n",
       "min        -1.000000       0.000000              ...                 \n",
       "25%        11.000000       0.000000              ...                 \n",
       "50%        17.000000       0.000000              ...                 \n",
       "75%        23.000000       0.000000              ...                 \n",
       "max       999.000000      39.000000              ...                 \n",
       "\n",
       "       addr_state_WA  addr_state_WI  addr_state_WV  addr_state_WY  \\\n",
       "count  830685.000000  830685.000000  830685.000000  830685.000000   \n",
       "mean        0.022308       0.012747       0.004003       0.002244   \n",
       "std         0.147684       0.112182       0.063140       0.047317   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       initial_list_status_f  initial_list_status_w  \\\n",
       "count          830685.000000          830685.000000   \n",
       "mean                0.504204               0.495796   \n",
       "std                 0.499983               0.499983   \n",
       "min                 0.000000               0.000000   \n",
       "25%                 0.000000               0.000000   \n",
       "50%                 1.000000               0.000000   \n",
       "75%                 1.000000               1.000000   \n",
       "max                 1.000000               1.000000   \n",
       "\n",
       "       application_type_Individual  application_type_Joint App  \\\n",
       "count                830685.000000               830685.000000   \n",
       "mean                      0.994908                    0.005092   \n",
       "std                       0.071178                    0.071178   \n",
       "min                       0.000000                    0.000000   \n",
       "25%                       1.000000                    0.000000   \n",
       "50%                       1.000000                    0.000000   \n",
       "75%                       1.000000                    0.000000   \n",
       "max                       1.000000                    1.000000   \n",
       "\n",
       "       disbursement_method_Cash  disbursement_method_DirectPay  \n",
       "count             830685.000000                  830685.000000  \n",
       "mean                   0.998617                       0.001383  \n",
       "std                    0.037166                       0.037166  \n",
       "min                    0.000000                       0.000000  \n",
       "25%                    1.000000                       0.000000  \n",
       "50%                    1.000000                       0.000000  \n",
       "75%                    1.000000                       0.000000  \n",
       "max                    1.000000                       1.000000  \n",
       "\n",
       "[8 rows x 221 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "# most correlated features\n",
    "corrmat = df.corr()\n",
    "# top_corr_features = corrmat.index[abs(corrmat[\"int_rate\"])>0.5]\n",
    "# plt.figure(figsize=(100,100))\n",
    "# g = sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "# sns.heatmap(df.corr(),annot=True,cmap=\"RdYlGn\")\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import cm as cm\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    cmap = cm.get_cmap('jet', 30)\n",
    "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\")\n",
    "    ax1.grid(True)\n",
    "    plt.title('Feature Correlation')\n",
    "    # labels=['Sex','Length','Diam','Height','Whole','Shucked','Viscera','Shell','Rings',]\n",
    "    # ax1.set_xticklabels(labels,fontsize=6)\n",
    "    # ax1.set_yticklabels(labels,fontsize=6)\n",
    "    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "    fig.colorbar(cax) # , ticks=[.75,.8,.85,.90,.95,1])\n",
    "    plt.figure(figsize=(100,100))\n",
    "    plt.show()\n",
    "\n",
    "# correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt                       -0.064021\n",
       "funded_amnt                     -0.064132\n",
       "funded_amnt_inv                 -0.063865\n",
       "term                            -0.177266\n",
       "installment                     -0.046476\n",
       "annual_inc                       0.046371\n",
       "loan_status                      1.000000\n",
       "zip_code                         0.016527\n",
       "dti                             -0.120715\n",
       "delinq_2yrs                     -0.021748\n",
       "inq_last_6mths                  -0.058934\n",
       "mths_since_last_delinq          -0.005921\n",
       "mths_since_last_record          -0.023344\n",
       "open_acc                        -0.034570\n",
       "pub_rec                         -0.023285\n",
       "revol_bal                        0.013427\n",
       "revol_util                      -0.072684\n",
       "total_acc                        0.011113\n",
       "collections_12_mths_ex_med      -0.018195\n",
       "mths_since_last_major_derog     -0.025663\n",
       "acc_now_delinq                  -0.007142\n",
       "tot_coll_amt                    -0.000020\n",
       "tot_cur_bal                      0.061666\n",
       "open_acc_6m                     -0.037827\n",
       "open_act_il                     -0.022859\n",
       "open_il_12m                     -0.035859\n",
       "open_il_24m                     -0.031631\n",
       "mths_since_rcnt_il              -0.001594\n",
       "total_bal_il                    -0.015194\n",
       "il_util                         -0.031631\n",
       "                                   ...   \n",
       "addr_state_NC                   -0.003622\n",
       "addr_state_ND                   -0.002090\n",
       "addr_state_NE                   -0.006749\n",
       "addr_state_NH                    0.010618\n",
       "addr_state_NJ                   -0.004963\n",
       "addr_state_NM                   -0.003390\n",
       "addr_state_NV                   -0.008473\n",
       "addr_state_NY                   -0.015443\n",
       "addr_state_OH                   -0.008578\n",
       "addr_state_OK                   -0.009697\n",
       "addr_state_OR                    0.015144\n",
       "addr_state_PA                   -0.003946\n",
       "addr_state_RI                    0.002230\n",
       "addr_state_SC                    0.008899\n",
       "addr_state_SD                   -0.001807\n",
       "addr_state_TN                   -0.007141\n",
       "addr_state_TX                    0.002480\n",
       "addr_state_UT                    0.006129\n",
       "addr_state_VA                   -0.000809\n",
       "addr_state_VT                    0.006318\n",
       "addr_state_WA                    0.014800\n",
       "addr_state_WI                    0.009154\n",
       "addr_state_WV                    0.003936\n",
       "addr_state_WY                    0.003163\n",
       "initial_list_status_f            0.017221\n",
       "initial_list_status_w           -0.017221\n",
       "application_type_Individual     -0.003934\n",
       "application_type_Joint App       0.003934\n",
       "disbursement_method_Cash         0.024749\n",
       "disbursement_method_DirectPay   -0.024749\n",
       "Name: loan_status, Length: 221, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrmat['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_corr_features = corrmat.loc[abs(corrmat['loan_status']) > 0.1]\n",
    "# plt.figure(figsize=(10,10))\n",
    "# sns.heatmap(df[top_corr_features].corr(), annot=True, cmap='RdYlGn')\n",
    "# sns.heatmap(top_corr_features.corr(), annot=True, cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Absolute Correlations\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(type(get_top_abs_correlations(df, 3)))\n",
    "print(get_top_abs_correlations(df, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/vjgupta/reach-top-10-with-simple-model-on-housing-prices\n",
    "    \n",
    "# c = df.corr().abs()\n",
    "# s = c.unstack()\n",
    "# so = s.sort_values(ascending=False).drop_duplicates()\n",
    "\n",
    "# with pd.option_context('display.max_rows', 1000, 'display.max_columns', 3):\n",
    "  # print(so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable \n",
    "def check_skewness(col):\n",
    "    sns.distplot(df[col], fit=norm)\n",
    "    fig = plt.figure()\n",
    "    res = stats.probplot(df[col], plot=plt)\n",
    "    (mu, sigma) = norm.fit(df[col])\n",
    "    print('mu = {:.2f} and sigma = {:.2f}'.format(mu, sigma))\n",
    "\n",
    "check_skewness('loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highly skewed features\n",
    "\n",
    "skewed_features = df.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Skew': skewed_features})\n",
    "skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print('There are {} skewed features to Box Cox transform'.format(skewness.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    df[feat] = boxcox1p(df[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a scatter matrix for each pair of features in the data\n",
    "# pd.scatter_matrix(df, alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running corr() function to see how the features are correlated.\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "# for y in df.columns:\n",
    "    # plt.figure()\n",
    "    # df[y].plot.hist(bins=20, title=y)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the data in features and target label\n",
    "loan_status_raw = df['loan_status']\n",
    "loan_status = loan_status_raw.apply(lambda x: int(x > 0.1))\n",
    "features = df.drop('loan_status', axis=1)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "\n",
    "# Split the features and loan_status data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, loan_status, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Fit the learner to the training data using slicing with 'sample_size'\n",
    "    start = time() # Get start time\n",
    "    learner.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # TODO: Get the predictions on the test set,\n",
    "    #       then get predictions on the first 300 training samples\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    end = time() # Get end time\n",
    "       \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples\n",
    "    results['acc_train'] = accuracy_score(y_train, learner.predict(X_train))\n",
    "        \n",
    "    # TODO: Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test, learner.predict(X_test))\n",
    "    \n",
    "    # TODO: Compute F-score on the the first 300 training samples\n",
    "    results['f_train'] = fbeta_score(y_train, learner.predict(X_train), beta=0.5)\n",
    "        \n",
    "    # TODO: Compute F-score on the test set\n",
    "    results['f_test'] = fbeta_score(y_test, learner.predict(X_test), beta=0.5)\n",
    "       \n",
    "    # Success\n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "result = train_predict(clf, len(y_train), X_train, y_train, X_test, y_test)\n",
    "print(result)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "result = train_predict(clf, len(y_train), X_train, y_train, X_test, y_test)\n",
    "print(result)\n",
    "\n",
    "k_fold = KFold(len(loan_status), n_folds=5, shuffle=True, random_state=0)\n",
    "clf = GaussianNB()\n",
    "print(cross_val_score(clf, features, loan_status, cv=k_fold))\n",
    "    \n",
    "\n",
    "k_fold = KFold(len(loan_status), n_folds=5, shuffle=True, random_state=0)\n",
    "clf = DecisionTreeClassifier()\n",
    "print(cross_val_score(clf, features, loan_status, cv=k_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"-- Applying more strict rule to find outliers -- \")\n",
    "for feature in df.keys():\n",
    "    \n",
    "    # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "    Q1 = np.percentile(df[feature], 25)\n",
    "    \n",
    "    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "    Q3 = np.percentile(df[feature], 75)\n",
    "    \n",
    "    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "    step = 3*(Q3-Q1)\n",
    "    \n",
    "    # Display the outliers\n",
    "    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n",
    "    display(df[~((df[feature] >= Q1 - step) & (df[feature] <= Q3 + step))])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/anilrajgr/udacity-machine-learning/blob/master/projects/customer_segments/customer_segments.ipynb\n",
    "# Apply PCA by fitting the data with the same number of dimensions as features\n",
    "from sklearn import decomposition\n",
    "\n",
    "# from vpython import *\n",
    "\n",
    "pca = decomposition.PCA(n_components=10)\n",
    "pca.fit(features)\n",
    "df_pca = pca.transform(features)\n",
    "\n",
    "# Generate PCA results plot\n",
    "# pca_results = vs.pca_results(features, pca)\n",
    "\n",
    "# print(pca_results['Explained Variance'].cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print(np.cumsum(pca.explained_variance_ratio_))\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pca.explained_variance_ratio_).plot.bar()\n",
    "pd.Series(np.cumsum(pca.explained_variance_ratio_)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features (df_pca) and loan_status data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_pca, loan_status, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Fit the learner to the training data using slicing with 'sample_size'\n",
    "    start = time() # Get start time\n",
    "    learner.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # TODO: Get the predictions on the test set,\n",
    "    #       then get predictions on the first 300 training samples\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    end = time() # Get end time\n",
    "       \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples\n",
    "    results['acc_train'] = accuracy_score(y_train, learner.predict(X_train))\n",
    "        \n",
    "    # TODO: Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test, learner.predict(X_test))\n",
    "    \n",
    "    # TODO: Compute F-score on the the first 300 training samples\n",
    "    results['f_train'] = fbeta_score(y_train, learner.predict(X_train), beta=0.5)\n",
    "        \n",
    "    # TODO: Compute F-score on the test set\n",
    "    results['f_test'] = fbeta_score(y_test, learner.predict(X_test), beta=0.5)\n",
    "       \n",
    "    # Success\n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "result = train_predict(clf, len(y_train), X_train, y_train, X_test, y_test)\n",
    "print(result)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "result = train_predict(clf, len(y_train), X_train, y_train, X_test, y_test)\n",
    "print(result)\n",
    "\n",
    "\"\"\"\n",
    "k_fold = KFold(len(loan_status), n_folds=5, shuffle=True, random_state=0)\n",
    "clf = GaussianNB()\n",
    "print(cross_val_score(clf, df_pca, loan_status, cv=k_fold))\n",
    "    \n",
    "\n",
    "k_fold = KFold(len(loan_status), n_folds=5, shuffle=True, random_state=0)\n",
    "clf = DecisionTreeClassifier()\n",
    "print(cross_val_score(clf, df_pca, loan_status, cv=k_fold))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pca_comp in range(1,26):\n",
    "    print(\"PCA component size: \" + str(pca_comp))\n",
    "    pca = decomposition.PCA(n_components=pca_comp)\n",
    "    pca.fit(features)\n",
    "    df_pca = pca.transform(features)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_pca, loan_status, test_size = 0.2, random_state = 0)\n",
    "    \n",
    "    k_fold = KFold(len(loan_status), n_folds=5, shuffle=True, random_state=0)\n",
    "    clf = GaussianNB()\n",
    "    print(cross_val_score(clf, df_pca, loan_status, cv=k_fold))\n",
    "    \n",
    "    k_fold = KFold(len(loan_status), n_folds=5, shuffle=True, random_state=0)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    print(cross_val_score(clf, df_pca, loan_status, cv=k_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pca.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process i\n",
    "mport GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    adaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "print(\"Original data\")\n",
    "print(\"=============\")\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print(name)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, loan_status, test_size = 0.2, random_state = 0)\n",
    "    result = train_predict(clf, len(y_train), X_train, y_train, X_test, y_test)\n",
    "    print(result)\n",
    "    \n",
    "print(\"PCA data\")\n",
    "print(\"=============\")\n",
    "for pca_comp in range(1,26):\n",
    "    print(\"PCA component size: \" + str(pca_comp))\n",
    "    pca = decomposition.PCA(n_components=pca_comp)\n",
    "    pca.fit(features)\n",
    "    features_pca = pca.transform(features)\n",
    "    ###### StandardScalar\n",
    "    for name, clf in zip(names, classifiers):\n",
    "    print(name)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features_pca, loan_status, test_size = 0.2, random_state = 0)\n",
    "        result = train_predict(clf, len(y_train), X_train, y_train, X_test, y_test)\n",
    "        print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
